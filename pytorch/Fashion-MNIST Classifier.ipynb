{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classifier\n",
    "\n",
    "4 fundamental steps:\n",
    "\n",
    "1. Prepare data\n",
    "2. Build the model\n",
    "3. Train the model\n",
    "4. Analyze the model's result\n",
    "\n",
    "We start off with the first method of preparing the data using concept of ETL (Extract-Transform-Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import torch   # top level package\n",
    "import torchvision  # provides access to popular datasets and transformations\n",
    "import torchvision.transforms as transforms   # interface which gives access to common image transformations\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "torch.set_printoptions(linewidth=120)   # sets the line width for pytorch console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. extract: Get the Fashion-MNIST image data\n",
    "2. transform: Transform the data into tensor form\n",
    "3. load: Put our data into object form which is easily accessible\n",
    "\n",
    "For this PyTorch provides 2 classes, **Dataset** class and **Dataloader**.\n",
    "\n",
    "## Dataset and Dataloaders with an example of custom dataset\n",
    "\n",
    "**Dataset** is an abstract class for representing a dataset. An **abstract** class is a Python class that has methods which we must implement, so that we can create a custom dataset by creating a subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subclass(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[:,:-2]\n",
    "        label = self.data[:,-1]\n",
    "        return sample, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An abstract class named subclass is implemented by inheriting the abstract class \n",
    "Dataset. The abstract class has __init__(), __getitem__() and __len__() functions.\n",
    "These functions can be customized to load our custom dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset transformation\n",
    "\n",
    "torchvision.datasets contains functions to create and transform a custom dataset or any of the inbuilt torch provided datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(root=\"/home/nikunjlad/data/FashionMNIST\",\n",
    "                                             train=True, download=True,\n",
    "                                             transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the arguments:\n",
    "\n",
    "1. root: the directory on disk to save the data\n",
    "2. train: it tells us we want the training data\n",
    "3. download: it tells the class to download the data\n",
    "4. transform: we pass a composition of transformations. since we want tensor format, we have ToTensor() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/nikunjlad/data/FashionMNIST\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_set variable as seen above contains information on the dataset which we just extracted and transformed. I has various parameters and information into it which helps us gain more knowledge about this variable. Let's explore the various functionality provided by this dataset variable. You can access these functionality or additional metadata using **.(dot)** after the variable name as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n",
      "---------------------\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "# this maps class labels to integer indices\n",
    "print(train_set.class_to_idx)\n",
    "print(\"---------------------\")\n",
    "print(train_set.classes)   # print all the classes present in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikunjlad/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/nikunjlad/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "# contains the actual training data, we can also get shape as below.\n",
    "# dont use test_data and train_data. instead use data. see warnings\n",
    "train_set.data\n",
    "print(train_set.train_data.shape)\n",
    "print(train_set.test_data.shape)\n",
    "print(train_set.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method MNIST.download of Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/nikunjlad/data/FashionMNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )>\n",
      "-----------------\n",
      "<bound method MNIST.extra_repr of Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/nikunjlad/data/FashionMNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )>\n"
     ]
    }
   ],
   "source": [
    "# gives download information of the dataset like number of datapoints, location of the data on disk, transformations applied, etc\n",
    "print(train_set.download)\n",
    "print(\"-----------------\")\n",
    "print(train_set.extra_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikunjlad/data/FashionMNIST/FashionMNIST/processed\n",
      "/home/nikunjlad/data/FashionMNIST/FashionMNIST/raw\n"
     ]
    }
   ],
   "source": [
    "# location of the folder where processed amd raw images are stored in .pt format (pytorch format)\n",
    "print(train_set.processed_folder)\n",
    "print(train_set.raw_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
       "  '8d4fb7e6c68d591d4c3dfef9ec88bf0d'),\n",
       " ('http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
       "  '25c81989df183df01b3e8a0aad5dffbe'),\n",
       " ('http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
       "  'bef4ecab320f06d8554ea6380940ec79'),\n",
       " ('http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
       "  'bb300cfdad3c16e7a12a480ee83cd310')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives the URLs from where the resource are fetched from. \n",
    "train_set.resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nikunjlad/data/FashionMNIST'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root data directory where both the processed and raw data lie\n",
    "train_set.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n",
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n",
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikunjlad/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/nikunjlad/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# target corresponding to 60000 images in the dataset\n",
    "# dont use test_labels and train_labels. instead use targets. see warnings\n",
    "print(train_set.targets)\n",
    "print(train_set.train_labels)\n",
    "print(train_set.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.pt\n",
      "training.pt\n"
     ]
    }
   ],
   "source": [
    "# name of the test file and training file\n",
    "print(train_set.test_file)\n",
    "print(train_set.training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is this training data?\n",
    "train_set.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# which kind of transformations did we apply on the data\n",
    "print(train_set.transforms)\n",
    "# both results are kind of similar\n",
    "print(train_set.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader class and its properties\n",
    "\n",
    "Dataloader is used to load the extracted and transformed data generated by Dataset class. This creates a dataloader object which is a form suitable for training, since data is batched, sampled, shuffled, etc here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f1c2cd30a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, once the dataset is loaded, we get a pytorch dataloader object. This object too has many options which gives various information about the dataloader object. Let's, explore them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.batch_sampler.batch_size)  # returns default batch_size\n",
    "print(train_loader.batch_size)   # use this preferably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.utils.data._utils.collate.default_collate(batch)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is used when loading map-style of datasets. (need to explore iterable stype vs map style)\n",
    "train_loader.collate_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/nikunjlad/data/FashionMNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "--------------------\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/nikunjlad/data/FashionMNIST\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# information of the source from where data generated\n",
    "print(train_loader.dataset)\n",
    "print(\"--------------------\")\n",
    "print(train_loader.batch_sampler.sampler.data_source)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# drops the last incomplete batch of data. If say 100 data and batch size is 30, the last batch of 10 is dropped if True\n",
    "print(train_loader.batch_sampler.drop_last)  # set to false, i.e no dropping \n",
    "print(train_loader.drop_last)  # use this preferably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel processes to spawn for loading data. Usually configured to be equal to cores present on a machine\n",
    "train_loader.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if True, dataloaders will COPY tensors to CUDA pinned memory locations\n",
    "train_loader.pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# timeout value for collecting batch of samples from workers\n",
    "train_loader.timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally used with multi processing.\n",
    "train_loader.worker_init_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a detour, so let's get back to loading the data using dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing with our data preparation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(root=\"/home/nikunjlad/data/FashionMNIST\",\n",
    "                                             train=True, download=True,\n",
    "                                             transform=transforms.Compose([transforms.ToTensor()]))\n",
    "train_loader = DataLoader(train_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)   # no of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many samples are there in the 10 classes of Fashion-MNIST. We have\n",
    "# balanced data of 6000 samples. If there is imbalance, then over sampling\n",
    "# will help. This is done by repeating uncommon occurences of data and balance our dataset\n",
    "\n",
    "# Oversampling is said to be the best method\n",
    "train_set.targets.bincount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVER SAMPLING is best method of balancing imbalanced data\n",
    "\n",
    "[arXiv publication](https://arxiv.org/abs/1710.05381) for oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set))   # create an iterator of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)   # get length of our sample. we have image-label pairs, hence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample)   # create type of our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample  # sequence unpacking (also called deconstruction of object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iter()** function creates an iterator object representing a **stream** of data. This object essentially holds a pointer and does not load all the data in memory. This pointer then iterates over all the data elements on invocation of **next()** function. This is analogous to the behaviour of how **generators** work in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some images to understand what our data contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "print(\"label\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)  # batches still contain images and labels, but now a list instead of single image-label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)  # its a list, instead of a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch   # we get first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape   # rank 4 tensor of 64 gray scale images of shape 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape   # rank 1 having 64 labels corresponding to above data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for training set which had data which could be accessed individually, the training dataloader has batches of data configured so that it can be accessed efficiently. This helps us in doing essentially the same thing as we did before but now on batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAB6CAYAAADDC9BKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO2dd7RVxfXHvxNLitGogICAioiCBbEjsSBY0KiIvWOPLTZiNJplVNRYli1GfxFL1Bgx9oaiiGAXhUAsoAgKNoo1iSYx0czvj3f38D3vzbxz73uX+86F72ctl5t5p8ydM2fmnLO/s7fz3kMIIYQQQgghRPH4TltXQAghhBBCCCFEHL2wCSGEEEIIIURB0QubEEIIIYQQQhQUvbAJIYQQQgghREHRC5sQQgghhBBCFBS9sAkhhBBCCCFEQWnVC5tzbrBz7i3n3Ezn3JnVqpQQQgghhBBCCMC1NA+bc24pADMA7ADgAwCvADjAez+tetUTQgghhBBCiCWXpVux7+YAZnrv3wEA59ydAIYASL6wOeeUpVsIIYQQQgixJPOJ975DuRu3RhLZBcD79O8PSmUZnHPHOOcmOecmteJcQgghhBBCCLE4MKeSjVvjYSsL7/1IACMBediEEEIIIYQQohJa42H7EEA3+nfXUpkQQgghhBBCiCrQmhe2VwD0dM51d84tC2B/AA9Vp1pCCCGEEEIIIVosifTef+OcOxHA4wCWAnCz9/6NqtVMCCGEEEIIIZZwWhzWv0Un0xq2wuKcAwBU0h969+4d7GuuuSbYd999d7CnTJkCAPjPf/4Tyv773/8Ge/311w/20KFDgz1r1iwAwGWXXRbKvvjii7LrVi+sssoqwT7ssMMAALfddlsomzdvXouO27dv32D36tULAHDvvfeGMr4G9Uz37t2Dve222wZ7yJAhAIBPP/00lN1+++3B/stf/hJsax8A2GuvvQAAgwYNCmX//Oc/o8cYOXJkq+q+JLLqqqsG+6OPPmrDmlSOjZFAZeMk3+MDBw4M9lFHHRVsG9vefPPNUPb1118He8UVVwx2//79AQAvvfRSKDvrrLOC/a9//avZ+rT0d4jiw9eWqeQ68zhq8zAAfPDBB83ux2PxpptuCiD7LCCEaMJk7/2m5W7cqsTZQgghhBBCCCEWHfKwLWFU8nV1o402CvZ+++0XbPNCfPvtt6Hshz/8YbC/973vBbtdu3Zl123GjBnB/t///gcAWGeddULZ/Pnzg/34448H+/LLLwcAvPbaa2Wfqy3http///2DfcoppwDIfln/5JNPgs1eSrOXX375UPbd73432F27dg32gw8+CAB48cUXQ1k9fvnceeedAQCnnnpqKGNvwrLLLhvsf//73wCy7cPe3I4dOwZ79uzZwf7mm28AAHPnzg1lf/vb34LNbdylS0MWk3HjxoWyk046qdyfUzj4d6y00krBZi/l0UcfDSDbZinMmzZ+/PhQ9v3vfz/Y7733XrB32mmnYH/11VcV1HrRk6c+aN++fbBPPvlkAMD2228fyrjPsLeW+6t5ebm/MuwRN08H91Fu188++yzYzzzzDICsAuLzzz+PnkPUP9/5zsJv8DaHNobnhiOOOAIAMHz48FC2wgortLoe9mxg4ykAnHHGGcG++uqrm92/nN8hxGKAPGxCCCGEEEIIsTigFzYhhBBCCCGEKCiSRAoAC2UQHPCiT58+wWaJwpdffgkgK0djyQ5LGJZeuiEQ6Y9+9KNQxpIn3javL7LUkiVAJi167rnnQtnBBx/c7LGKwj777BNsa8+zzz47lHGQBpbxmcyK5U12XQBg7NixwR41ahSArBTzgQceaHXda0GPHj2Cfe655wLISmN/8IMfBDsmo2FJTrdunDYSTbZlm2WQfAy2TSpo0kggGxjn5z//efR8RWXChAnB5nZnSZ/dd//4xz9CGQez4ftuqaWWArBQngpk24fHjw033LA1VV+kxCSR3D4PP/xwsK1v8m/msZFl5Cx9Nhkj36OpbW2869ChQyizcZb/zjZLMa+//vpg33fffRD1j419KfkgB1lae+21g233NvcPnp95zrW5hu/hzp07B5vHYjsez9Pct1m2++STTwb7oIMOalL3xUUeyctRYr8p9fwTCyRTyXO7BSkCgBdeeCHYvNyEl6PUWyCi1rZPOXCwsSuuuAJA9p7iOZLH6jKQJFIIIYQQQgghFgf0wiaEEEIIIYQQBUWSyBJ50RM5etdWW20V7Mcee6zZY5ksCMjKqSqpT3P1qhYmS1h99dVDGUeHi8kc+fek8r+Y659lQSwHiG1bDrHrxfKMwYMHB3v69OllH7fWsARkwYIFALJ5mzjqIEfuMxc8y1MmT54c7D/84Q/BXmONNQAAH3/8cSgbM2ZMa6teE6677rpgm8yM+2IqOqn1TZb6cH9lySPvZ8dmiQPDMjU7HsvfOBIly4tHjx4dPV6RYGmj5VECsm248sorA8jK8fi+taiEwEJJNUtYWbo3Z86cYHN+snrgrrvuCjZHiTSp1zLLLBPKeNxOScdNRsNympgMElgoL+dz5I2/vD/vt8ceewSbJdWi+OQ9s3BUYL6f+X60fsH78zMLl5vkke93Hht4bLQ+lsoJyH2Q7x+LaMz9kqnnHIIpSSS3W2sZMGBAsDfYYAMAQM+ePUMZL3Ph+uy4447BrlDSVzXyrm05176S52bugzYuW5sBwD333BNslhHH+iiPrxzNuwwkiRRCCCGEEEKIxQG9sAkhhBBCCCFEQVk6f5Mlg5iLeq211gplRx11VLDZzW8RlVgW9fLLLwc7JYM01y2fl925sf1YqlANN/omm2wSbJNCcqJmli/xuS3yE0cwTEXrM1czH4vrzr+ZXdT2+zkanSWM5b8zfNwjjzwy2EWO1scyJJOGcELh0047Ldic8NQkae+++24oYwkry0ys7VOyqSJzyy23BNsSZrO0k+U9LFtm6ZnBUgWW9DEmlUxJeWLH4wio77//frDrQQbJvPPOO8Hu169fsGPRClN9iRNqb7311gCADz/8MJRx1DgeM+oBllx36tQp2H//+9+DbdIYHp/4dy633HLBjkWK47Zmm2W7doyYPLdxuY0vPD9xHXbfffdg33HHHRD1Q0zqNXTo0GBvscUWwea5k/udzbmpaM1s21yckvZxufVBvt/5HNxfeb4zad7OO+8cynjZSZFlkLFosgyX5z2/HXroocF+6aWXgm1jKi+V+Oijj4LNkse3334bQDaa4SmnnBLsqVOnNluHWsPtkydt5OdRxvojP2/yXB57NgWAbbbZBkA2ai7//c033wz2CSec0OS8seeNRYE8bEIIIYQQQghRUORhKxHzXvFC+O233z7Y/LXKghPwV9Qddtgh2DfeeGOw2RtgXwtSX1o4mIJ9meIFvtVgu+22C7b9Dg62wF/EuH3sa+0ZZ5wRyvgrD7ePeeHmzp0bylJfOXjhpv3+jTfeOJT97Gc/C3bME8j13XvvvYNdZA9bzFPYrl276Lb8m+fNmwcg2+84Hxj3K+trRf46mYK91baInr0CEydODDZ/VbN2Ya8je9jYS8feB9uPj8UelJhnjq/BmWee2ezvKTIcnCcVeMAUBdyW/FWXsS+b/LU01a71AAf9YQ8b32s2hrEXi+/x1PgaU1ykvijbtrH9G9fH+iuPHTzO8rwmD1vxyVPZsIeArzmrDzhQVUwBk+p31jfLmUdizzcpDwora0zh8Oijj4Yy9mzbvMd1riSYWxHp3bs3gOw14OAhHDDGgj7deuutoezpp58ONnvTbD/en8dtVpDNnDmzxfVfFOT1sdRzs5WnPF48ZnJeVutvrOjivj98+PBgm2KkLQLgyMMmhBBCCCGEEAVFL2xCCCGEEEIIUVAkiSwRy52w2WabBdtyWQFxmcDjjz8eyjbaaKNgX3rppcGeNGlSsF977TUAWRnS5ptvHj33Cy+8ACCbV4XzSLUUlg2arCAlueBF73buG264IZRxHg8OZnLzzTcDAH7605+Gstdffz3Y5uJvfG7LSXbllVeGsuOPPz7YLB+wurFktFevXsHmHBozZsxAkYhJoLjduU1WXHHFso8bc9dzm9Ujv/3tbwEAJ598cijjBessczTpHvcJljsw3C52DC5jyQ4fw4KN8KL4epP5MamgPrEgBSxxZhkOt49JR2JyPqA6Y1gtYekn/yaWR1pbcZux5Jal47NmzQq2BWuxftt4Py43uQ/LKzl/0G677RZsk6Xy2MFye5ZuiuKTkoJZbiiWO3JAK86vytuYRCwlK6wkN2qMVACT1Bxn/ZwDRbA88M4774weowjkyeJYOt+/f/9gm8yTx8Obbrop2BZsC1g4flxxxRWhjPO2ch0sUAYvK+HlOjy+FE0SGQvIlKJjx47BtudJfq5kSShvy3O85c9kyS0HE+Nn97ZEHjYhhBBCCCGEKCh6YRNCCCGEEEKIglLfGqlWkoryYm5jdqWy1IdlJCa3Y9ndK6+8Emx2NbMUxVzie+65ZyjjyDZ8DMsBx7LNp556qplfVh4bbrhhsC1/FMsTWHLDrLDCCk3KxowZE2yW71gEJI7UeP/99web5TvsojaZFcsrWbbB18CkEew6Z6nclltuGeyiSSK5T1h7s1SBr0csamcqHxZLCsxmWWu9wH3Crv9WW20Vyi688MLofiaF5D7DOYFYchPLMWj5xoC0LMjKH3744ZxfUR+wzJHHGu5j1ge5j06bNi3YLB+19mGpD48p9ZYXkOVYzz77bLAPOuigYK+//voAgIsuuiiUcQ6fFCaX4j7KNo93dh/zOMsRHn/5y18G2+YRlgKxTHjNNdfMrZsoPjzHGRwNNBVF1EhJF5mW3K+p46bqY3XmuYqfw/geLFrUY5tHUjnteK7n+cXGDJZ+8hKSwYMHB5uX3hi2fKQxJpU0uR+QjSR9xBFHBPv5558PNi9ZaStizz09evQIZVdddVWwWe5tz+nrrbdeKOM8oFw+YcKEJtvwPcPXqKXLSex3VEu+m+thc87d7Jxb4Jx7ncpWds6Ndc69Xfr/Ss0dQwghhBBCCCFE5ZQjibwFwOBGZWcCGOe97wlgXOnfQgghhBBCCCGqSK6fz3v/jHNujUbFQwAMKNm3ApgA4AwUlEpd+SNGjACQTdjIcLQfk1yxhIglW+zOZ1f5lClTAABvv/12k2MBwIknnhjs7t27A8hGdWwpHE2Mo+rZuVn+FZOKAdlkxIa59YGsK9nakKVrfD1YBsrlMYkHR1hj1761K7cvS7a23nrrYHPCySLArnb7/dwOfD24PJZol/8ei/LH17NeiEUvY+keR9qz+wRYeP1ZypzqH9yGFlmNE2SnIiay7HZxgMcDjorLkj5rt1QybMbu7ZT8KZXctKhwxF/uS+PHjw+2jessG+f247bgiKI2psaSGgNxORlHMWOpD98TJtfkiIE8fvNYXQ+k5vJYsueUNC0ms05RSbQ6hqXBdo5FKeEziTdLulIyrNj8G6svkJ0zrP6phO2xiMepNuNrwH3Q6s+yXZYc89KKomG/NXWdWYbP7TZw4EAAwO233x7Kjj322FbXp127dgCyY9HkyZODzc+sLFW3/WLPebUiNjfwuHbYYYcFu6X15PnOJLgWvR0A7rrrrmDzs2dsfElFQq52UveWrmHr6L23p6Z5ADqmNnTOHQPgmBaeRwghhBBCCCGWWFoddMR7751zyU9H3vuRAEYCQHPbCSGEEEIIIYTI0tIXtvnOuc7e+7nOuc4A4mFqCkKlUoTPP/8cQFYSye5sdh+blIAjALHciqWE7EI12SRL/1hSwMkQOQJjaznjjIXKVa6bSWZYRsF/599kbl6We5obHcgmLbT24Shl7O7m47KcwyL/7LfffqFspZUWxrbh62HSIHZF87G4nkWDr7nJQGKJ2YHKIn0x9SZ7qgRun+WXXz7Ydq/xvcrySO4f3AdZJmKkpEXz589vQY2LCycNZWKJs1ORM2PSM+7PLIWycbZe4AhtgwYNCvZee+0V7B133BFAVnp9/PHHB5tljGuttVawbf5ISfdYsmZ9lOcTllNxP7fxnvs1tztHKeZkvhxZrkiUM5fbOJnaNk+mxNfr7LPPDjbL8POohdyXozy3b98eQFZmy5EW+fpzuY19fD/zeBeLEJuK9hiLCBmT8QPZa8DlNsdzfastK1tU5PVNvi+feeaZqG2knr1i50hdA3t+5fud+8djjz3WZFtgYZL1tpRE5sF1iy3jKef+Yym7jYPcVttuu22wL7nkkmDHngdSzwjVlpe2NA/bQwCGlexhAB6sSm2EEEIIIYQQQgRyPWzOuVFoCDDS3jn3AYBfA7gYwF3OuSMBzAGw76KsZK2xoCIpTwcviLUcQ/xFkhfs5y3Q5QAm/JbO+3Xr1q3yH5HghRdeCDZ7vexrLy9Q5dw/HBzF6vnSSy9F68u2bZv6yp7yGln78FcpzqHGdbNj8zXiRaIPPPAAikrMU5GXey21HxNb1M1e23rEfjO3CedY6dOnT5Nt2bvI+/FX5lg5e3D5Cyd7kvncRiUBDYpMyisb+8LLZbFxIPXFnr/21gMXX3xxsPkLLo8106dPB5DNL3nOOedEj8fHsPbmtuJ2jQWCYC8xj4f8lfjll18GkPWe8pdlzhNaVK9aipRnIe++O/DAA4Pdt2/fYO+zzz4Asvf+J598EuxRo0YF+4ADDmj2HHxtfvGLXwAALrjggmb3qRQea6xPcDvEcpUC2XYzz205ga6sPOVhi3njGD4ue9B4XrNrx/Xt2rVrk2PVO7E5Pi/fJ1BZPi8LnMUBh1LXmRVi9TBvpe79mGctNSffdtttwbZ7n9uEFRCpHK7GuuuuG+xrr7022PaMcPDBB6d+SkWUEyUyNTINSpQLIYQQQgghhKgCLZVECiGEEEIIIYRYxLQ6SmQ9kHIDs3uZXcKrrroqgHQwApY7WPlXX30VynhhOS82ZPmjHYPd1SxHfPXVV5vUjYNnTJo0CS3huuuui9q22Ldnz56h7Ljjjgs2L8A06czrr78eyjh/EC+QryT3V+w68TXgdv3rX/8abM7TUg9w8JRY/g528edJHxmWobAMwNqQJTKxhef1yOzZs4PNbWX3F7f1nDlzgs3SCJY5mpyM/873Pp+jHqQjLSUv51Q5sqhY4AHej8fMeuD+++8PtuVOArLjsi3kf+ihh0IZS5E5dx/f+zZmsvQmNXZav2NpPkuBOPiOBRA45ZRTmpQBwIABA4JtOeQa20UgNjamgjzYHGYyJyAb3MsCwwDZ3E4ffPABgKxUl5c37LLLLmXXd//99w/2FltsUfZ+lbDxxhsH2/pPau7gMYwlXfZskQrSEGvvVLvH7nPuw6n+zPW0/s9LIfgZidty4sSJ0ePVA3mBK/gaxdotNaYyNt8PGzYslD3yyCPBvuOOO4LNbRyT/BWNSgIJpuYybgt7puVnTFvuBGTHexsn7rvvvuhx+ZmD5dfVQB42IYQQQgghhCgoemETQgghhBBCiIKyREgi2X3K7mV2QXO+L8tJsWDBwvRyqahy5nbmSI4sP+A8UCw7MMkaH5elWRxpxiJZscyt2pgUzKKKAdlIcewStvZMRSlLRTk0yoksZe2Syh/D0S7rDW5XtvPc/Hk5WFLySbse7OKvZxkkw7KwWF/jMm6f1P1s94FF2AKycmmG+//iRp4Ul/tdntSJ+y2PufUWtbR3797BZtkQR2C0yLk//vGPQ9n6668f7NRcZHBfzJOdpsZZro/JnqZOnRrK3nnnnWC///77wX7rrbea1KfacL+yOseWGDQmNvZZrk4AuPDCC4NtczmPDXPnzg02z3Es3zc53ptvvhnKOELhiBEjmtSB+zA/Q1xxxRXB7tWrFwBgk002CWWTJ09ucqxKiY393A/KyUVlx+Bt+ZklFqU4NabG4OvGx+W5KPbswHJz3o+lvXmROhcV5cgRqwmPmbExIxU50iKcsryZ5dvXX399sHv06BHsoj5bldPutk2l18hkjrwsiaWNLJ+04/H7Ad8/EyZMCDaPO9VAHjYhhBBCCCGEKCh6YRNCCCGEEEKIgrJESCJZSpiSXHDEQ5OLsVQjJuUAFkoiWGLGkSFZcsEyLJMBcJJTc8sC2egyl112GYBsoupqEEugye3DrmSO2hSTRuS5qFsqHUjJrTgqZWzbcurWVuTJoqp9DpaU1DMxySNLZz7++ONgWz/m+4vhcu7zJouaP39+KGN5JEfTWpzhsSFWnpLi8vWwbVKJSzkCXz2w5pprBpt/E8vmTI7Icjz+zTyOxtqtnDHM2pWjDrMkh/ur1YMjR3J9WVbYqVOnYLNssrWkJPBGak5mBg1qSPu61157hTKeI3nOnTZtGoBsu7PUiZcesLTV2oplYywv5fOdfvrpTfZ/7bXXgs1jrs37fO2rQWwsSkWG5P6RJ6PPu17lEJNa8j2TJ5VM/Q5+hmor2vJ5Ii9xNieCt0jjd955Zyjbddddg73TTjsFm591WSZdJKoRGTLFhhtuCCAbnd2ixQPZqK82lpx33nmhjGW9Y8eOrejclSAPmxBCCCGEEEIUlMJ62FKL2u3LC/+dv+LkfYVP8eijjwbb8gPx1zP+AsFv+vZVn+vIX4FSC3+tPLaoFwD69OkTbF6gW034d8TqyTlquA72pYwDZqSOm+dhS33Bs2Ozh5LhXDlGKsde0Uh51awvlJN7rZJtbRtuk5THuMjEFtanFgnb13L+ms6wN449FZaHJfXVn9tttdVWa/L3xSU3W+q+jI2/efulAj3Vm4eNrz0rKvg3mReF+1RqjI/lYEzdl7HgTLG8g42Pa4EHmJVXXjnY7PXgL8rV9LClgs7EOOmkk4J97LHHBrtjx44AsioUVsXwfWfbMilvZay9eWzg8YWxwAxDhw6N/v1Xv/pVsI8//ngA2Rx8Bx98cLBnzpwZPUYeZ511VrBt/k4F6+Brzn2ipR60PGJzDl8DrhvP8Xb/cD5C9lbvsccewW6teqdeSI2fxhlnnBFsvs6///3vAQCHHHJIKGNPND/z8lhcjse7SMQCjPC4xm2WCuRkz5v8XJl3b5x99tnB5mt09913l133SpGHTQghhBBCCCEKil7YhBBCCCGEEKKgFEoSmXL9tlZmtM022wSbFy1zrhyWP5rbmGUmKRerueu57rEFx0DWHctufoPPZ7JMANhzzz0BAA8//HCTfapFTMLAbRLLLcfXhdsn5qJOLWRmSQq3j7moWVrE+xVZ8phHqk/E2iolY8wLVhK7Bnwu7mv1kpMtJt1k+RJLpGzhNPcf/p0sm+K+PWfOnCbbsiyK86p06dKlsh9QcNZee+1gc//gdo/lguR+GbvPuYzHjPbt27eyxrUl9TtjefxY0pXKWxWTcsXGg8bnM/kbzzN8XfgcFjwnJeHkcYQDk7SWjTfeONg77LBDsNdZZ51g2zjIUkzOeciBpT788EMACyXLQPb3c7m1G8+xLLtLja/Whny9eA7kNtx8880BAB999FG07izdfPvttwFkx6Kjjz462Cxpq4Tu3bsH2+ZLbhO2bVxrXI9FLSvktuZxltsqFoyE+yX/ffbs2U22XdyJycjPPffcUMbtw7nB7FnX+h+QvQ/4vquFDDIm+07JDvkebMmSjXKCzr3yyivBHj9+PIBsIJYUNjdyH+X7KyZDrxbysAkhhBBCCCFEQdELmxBCCCGEEEIUlEJJIsuRuVkUHHbnspSnc+fOwTYpIcswWNbArlmWIFpkOZY78H4sF7I8bOxSZsmBRZMCsjIAk2my65YjMfLx+vXrh0VNzG3MdYtF2klF24odIyXhy5NHpuREMTd5vUgkUvLQlkbUrOR8RjnRJeuBrbfeOtgc2S4mbeQ8SCz/4lxUJqPi+4/HFMZklTYGAFlJSr1F4uzdu3ewWdLFEWRjUVtj0Q4ZbgeOLMuy1P79+webx8yiksqXZnm7WBKZIiarTEkbY3ZK2shYe6fG2XKOUS4nnnhisG3uBbJtEZPIcZ9iGSNva3MntzXP2SyfjEkbWYbOx2XZoP1+ri/vx/W0aHLcfpzbkaW/drxqSE5Zhs3PGSbD4jIew/Lm0dRYFZNDp+aOWETI1L3PElYeX2y8Zhk6t2W3bt2i564meVEZq30O64/8XMn3AY/LlpN3xowZoYzbZPjw4cGOPUdwnjbOK/niiy9W9gMa1T0l345JvGuxnCU13957773B5ryJhx9+eJNtU/eE3Qd8r02ZMqXlla2AxeOpTQghhBBCCCEWQ/TCJoQQQgghhBAFpVCSyC233DLY559/frA7dOgQbJMvpaQcLI0wVzonw2OZALtuORqUSXL23XffUDZp0qRgs7TB3PypJLAbbLBBdD+LYseub5ZisHxy9dVXjx67lrAUw6Qf3O4peWRLZXx2DJZLpJKp1xstrXtKdhAr423tfKnIW0UmJktgCci6664bbJZEWhJtTpzNCWqXW265YHO0NRs/UglzmS+//BIAcOCBB4ayq666qkl964VBgwYFO+9+LkcCY6TGiVmzZgX7uOOOC3ZRJZHlSJVtbGT5HO+XSoZtc1UqimTs3CwVS425NqfwvMgyQIblfy3hj3/8Y7A5AhtHY15vvfWCbfMaz4t23wLxyMz8O/m5gO2YDD8V8Tkm77P7GsjKLvnZwdqer0tq2YQdgyWBo0ePbnLecmAJOGPtw+fl+nLdOLmyza+pPpoXybQSuD783MPns77A14jrXot5PybZSz3HtLQtYs+v3Cb8vHXaaacF+6mnngIAbLHFFqFsn332Kfu8seeCxueuhFhk60rapFevXsE+4ogjgm3STyAbCdpIyRVtDOM+c8EFFwSbly9wxPgYqfk7Nr7wXMZUOwprrofNOdfNOTfeOTfNOfeGc+7kUvnKzrmxzrm3S/9fKe9YQgghhBBCCCHKpxxJ5DcAhnvv1wXQD8AJzrl1AZwJYJz3vieAcaV/CyGEEEIIIYSoErm6KO/9XABzS/Y/nHPTAXQBMATAgNJmtwKYAKBFGSDNtXj11VeHMo4CydIPcyWnXLgsCbBtWe7IcKQilh1efPHFTfZjyU4seuS4ceNCGUuzevbsGWyWZ8UiZLGbl39zzCVcbfJctjGZQKytgfzkuSk5FbugrV1YRsL7xaLV1WOUyJM0738AABZgSURBVFj0zVRENyZPqhLbj4/LfZ8lw0UjJkvg5JbTpk0LNku6LOIq39eWfBfISjH4HBYdsU+fPqHMkg8D2XvY5G8sX+H7nROW1gMcjZalyLGIZilpTQzui3yN+N5mOfziAP/OlAwyFQXSSI1nsQh8LDfjcpNEcl/kSHGpJQItgffnJPYTJ06Mbm/STJYkr7XWWsHmZQb2PJCK9hiTSHECW5Y5fvrpp8FmqaiNGVzGzxmxZw6eA1PtZ/VgeWVL5yq+Lxm7l1LSWI6Ey9vY8VISs1gfTfU1xsp5bEhJNLnc5Jq8Hz8LtRXVeLZIPQPFnq04MTY/b9q8tN9++7WoDnxt27dvH+xKEmfHktDzcfl6sRzxqKOOArAwkm5jeBwYMmRIsDnKu5EaU61f8bIJlozusssu0XPbOMnP/Kl7wmTbXPbcc89Fj1ttSWRFC1mcc2sA2AjARAAdSy9zADAPQMfEPscAOKblVRRCCCGEEEKIJZOyX9iccz8EcC+AU7z3f2/0tcA756KvkN77kQBGlo4R3WbYsGEAsl/DeREfB+AwmxfOMvz2b14EC/ABZL9WcB4F/op+6623AgD22GOPUPbwww8Hm78EWPCCTTbZJJRtt912wU59BbWvi/yFjuGvFPab+KsB/6ZaEFv4y3VM5SWyLwupL2385YEXGlt5ypPKXwzrjZRXtZJAIpVgX/B4/9YGGGhL2Pv16quvBpvb0u6vVICFVNAV67vch7nv8z1onkn2UPIYVm8eNvZocE6pVCAMIxVUJAZvy9emU6dOTcrZA1cEOI8fB62JeRk4gFRqvMvLJZnK12htyNumgjNZ3d57771Qtummmwab27i1AR3YM8Xtw3kMY2PcZ599FuwJEyYEm8eomGcpL+gV758KQMJjsW3PzxsczISDo9h+XC8eU/jZwvoNb8vXg/NB5fH0009Hy2N5S9lzk1Lv2PVPtSX/JtsmpaaJeeZSfYrrw+ewduU61lo5E5tz+XmD80dy3+a+GyPvd5x33nnB5t/P893QoUObPUZsXks9p7GHrRJSXt4YG220UbCt3VJKIM5hyvfdbrvtBiD7DM7E2nXUqFHBHjNmTLBTwUFSKrwYNlfxs2mtAmWVFdbfObcMGl7W/uS9v69UPN8517n0984AFqT2F0IIIYQQQghROeVEiXQAbgIw3Xt/Bf3pIQDDSvYwAA9Wv3pCCCGEEEIIseRSjiTyxwAOAfCac25qqewsABcDuMs5dySAOQD2Teyfi8kRWebHeZBYkmTbsGyBJQ68n0kt5syZE8p4P3aD8jnMhXz//feHMpYtsHTIpJkse2FpCLuPWUoQCzqSWkhpv2/ttdcOZbWWRObllMqT7pWTmy0mB+Iydu2z5Ki58xYRli3kSZ1aSmyhdko2VS+YFHnu3LmhjGVPHFjA2jivzzTexvp5SkrJMgiTeLDMmqUc9YItomaJDMtTuC3y8u7EZFixsQwAnnjiiWDzwnCTlxclH5vVOSXliQXt4XE9JSGKBVFKyc0Y69u8bSp3lm07e/bsaN34GLFATi2FA2ywHYPvy1TdbN7mvpiqr42jfI1igR0ab2PtxteTAxXx9bB2TdWXpWdWzu3AY0Yl/OQnP4mW2/MEP4fwWMRLPmJBQ2LLEbjuvF9KIs2/2bZJBQpL5VaLyS5rnc8yNv9yvs+YLB5YKIOtNKeZBa3q379/KON5LZV7L0YlkuvVVlutonoa22yzTZNj3HPPPaGMry0HDzQsuA+QlUPz8zjfK5bbNCWJZB58sMFvxPkeOYBJNbClVuVc52o/Z5UTJfI5AKmzDkqUCyGEEEIIIYRoJWWtYRNCCCGEEEIIUXsqCuu/qDDZAbtrWfLHEadMtsOyQ863wjnLzM2fklGw25kjQJk7n4/bu3fvYLO71urJUdX4fHwMlsaYDIvLWBrCUdPMhcz5czjvWy1I5Vsx8mR8lUoiY5IKlq5xFK56IxUZNBbpK6/dy8GOy32N76l6waQo3D4s5eF2tXubpTWpyJAmCQQW9jHelu1333032JZzjeVGnN+OI9my9KNoWCQvvi/5XuNx0vpSKrcaXwO7Tql7mPPrcBvbWFsUSaTVPxU9j2VzRirqXkreFctXmYrAF8vDlpKx2bw2Y8aMaN1TUSlrCUuhUtHaeH5dkhk8eHC03MZ2jvrJzzScR/b2228Ptt2vHAGV+xrLJ61fpfpdrJ/zsxCPEzxOcuRLi7LLz3cpTJLO42+KSvJhxeTei3IsGjlyJIDskpddd921RcfKk1TzteVcpJWw5pprBvv6668HAIwYMSKU8dIElkRaOT+HsCyT85nG5MWXXnppKLvxxhuDfckllwTbIrSPHTs2lHHexWpgkUHLyV9b7WU68rAJIYQQQgghREHRC5sQQgghhBBCFJRCSCKnTm0IPslRGQ8//PBgc0Sld955B0A2Ek0qYqS54LmMpSosH2AXbCxp87x584IdS07JMpNU3WKRJFMRJVk6ZNHxynH9t5RKXLd5CVbzZDap/WP7pSJ9tTbJa1vC/TEmF6uGNCnWbty/evToEewpU6a0+ny1wO4x/m18j7JM1qTPfM/FZGVA9h61a8BjA0s1Jk2aFGyLlsVRK3kcYKllkSWRJr9Jybdj7cZtxv01Jjln6Qgfl2XfPN5tsMEGLfgVi55UlMiYJDIlG+P24W1sPMuTTPLxyokoadKzN954I3relARTFBOeO2KJ3FP9h5+trrnmmmAfeOCBALLyyXbt2gWbn71ikXNT0UntPufjcn+dOHFisK+++upgb7vttk2Om4rwufvuuwMAbrjhhujfmUqeb2Lb8r3x6KOPBpvnhosvvhgAcMcdd+Se45xzzgm2yVy5HSpJpl4J/NzE81Ml3HLLLcE++uijAWSjMvJx+drZMzQvx+CE5Dz/sHzW2v70008PZWzzMiiTVJ977rnRuvPY19Loo1bncmS71Y5wKg+bEEIIIYQQQhQUvbAJIYQQQgghREEphCTSuOiii4JtMkkAGD58eLBNHshuUHZNcgRHc/+yjIAlS+wejklOWL6Sii5p5Sk5CZezpNEkRRxJjt2nLBd69dVXAWSjO1WbvChKLC3Li9DIvyOWCLMct3QsQW+eJLJeEmfHkkkC8ciYsbZsvE3j/RvvZ23IsjOWH9QLJtXh+5nHgfXXXz/YMTke78dtwbId24ZlzX369An26NGjg23jDh+X5SCpqJRFw+Sx3A48/nC/Mmkn/3233XYL9iOPPBJsk6fweMEyLoZlMiyvKRIpSeR7773XZFuW1HIf5d8fS26fGidj0kUuS0Xjs3kmlQA6FXFVFBPug3y/liPPMs4888yoHSMWSTv2rNTYtueFciLpxYglKQeyUURt3ClHEjlgwIBMvYDsvciSdX6GtPuY5wO2eWnBaaedBgB48sknQ9mCBQuCveOOOwb7pJNOCrZFycy7FpWS94zAY1RLmT17NgCgX79+oYwjvPNzs0X15DpwREkew2J152uUqrs9Y6ckpZU8I3J9uN+ZzDy1RInvGe4r1UAeNiGEEEIIIYQoKIX4pGZv3Py1jxd2sj1w4EAAWW+c5e4Asvk97LjsmeCvNalF2/bmzG/j/IWS3+7tC0E5gTR4wb0FS+CvDZw7Yvr06cEuSj4iw+qcaj/+TWanvsrl5WRL/b2eg47wVxf+AmW/OeX5zfMwcv/iv8cCRcS8AkWnQ4cOALL9i3Os8L1v9zkHBGFPGOd14i+qeXnv+IugHYPHLT6W5WsBgLfeeqvZ47Yl5hWzr9BA+n7lXJEGtwljHiT+qs1wf+Z7YlEtuG8pMY8WE/Ma8tdZtvke5eAO1lbsdctTbXBfZU8GeyutD3L7phQnqfyQojgcddRRwd5rr72CbV7sVJCulpLyLC0qLM/lKqusEsrYe8jei+eff77s466xxhqZ/wML5xMAWGGFFYLN96h5dXiMZw/Sn/70p2CbEmr77bcPZf379w82B1PiupuCjMdJHjOq4Qkz2FP0+OOPt/p4FmjFgtcAQNeuXYPNY5jNE5ZXGEgHBYup22JBmoDsc81BBx3UpI4tDTSSGn+tD7L3NHW+aiMPmxBCCCGEEEIUFL2wCSGEEEIIIURBKYQkshI35VNPPQUgu8iR6dWrV7DN5c3yJ3bXzpkzJ9jsmp01a1bZ9VlcyFuMyflY1l57bQBZ+U4qx5W5s1N/5/OyhCO2AJ63reegIy+//HKwrS2Bhfk9WLbAsIve2r6c32yyKG73Ikv0UpjUi3OvpXLJmGyB72vuUyyH4aAQdg7+O9u8yNzaMyW54KAARcYW7Y8cOTKUcV/jADWxsTo1ftt+LFVluRG3D0uSOB9REbCxhvtSnnTx3nvvDTb/Nu5rPIbF5GspabTZ3O5cH5Yccd7A2LapICeimLA8kJeC2LIJ7mujRo1q0TliSxrYTs05sfLUvJ+SXJtMj6WfPE7w8phLLrmkmV+RhXOH5cFSZXte5OBwKcmfXQ+WQfL14LpzrjaWWBrVlEEy/Gxx6qmnBnvEiBEtOp7J17kdLK8cAJx//vnB3myzzQBk26QaPPvss8EeP3581Y6bmtfs+vIzMbMon0M1QgshhBBCCCFEQdELmxBCCCGEEEIUlEJIIqvJm2++GbWN119/vZbVWWwwuR6wUDbGErP27dsHOyajKCcCGUt1TA7EcgHO58TStNh5K5HZ1hqW9N12223B3m677QBk25IjvrFEKpbDKfV3i7zFcgGuQ73Qs2dPAAt/D5CNGsZYX+A+w1HOOPIqR7iyPj1u3Lgmx2ps2z3BkSG5btWUZ9QCzjdnEc8aE5PqcEQ3xvLucGRJHjNY6rTTTjsFm6XqRcDqn9cPmN/85jeLvmItJBWlN/Y7RHHhSL82v/I9xdI9hucUHruMlIyxmqTmKsu/y9JpjgL4u9/9bpHUh+HIw2wvDljeNAC49tprF8k5xowZE7UNXgayySabBJvnny5dugTb5KipqO3HHntsk3Okck1WQkqWeumllwJILytJRUWuBvKwCSGEEEIIIURB0QubEEIIIYQQQhQUlxfRxDn3PQDPAPguGiSU93jvf+2c6w7gTgDtAEwGcIj3vllfoHOuPsL4LYGYCznVHy677LJgW1JHjljFiQ4Zkw5xct2UJCcWdZKlESzZ4UiLlvi3XkhFyIrB0ak6deoUbIu8x/vPmzcvascSnlZSh6JgcjruJykZrElmWV7XrVu3YLN0UTTP1ltvHezevXsDAAYOHBjKONoYJyq3MYMlk3/+85+DzVHT6oHLL7882Cy1HT16dLBtLEolXS3CvXbhhRcGe8011ww2y7Mfe+yxmtZJVA73sUMPPRTAwkTPQPZe5GihLEuOSetrQSpC6p577glgYeRaICsxGzZsWLCfeOKJRVlFIWrBZO/9puVuXI6H7WsAA733GwLoC2Cwc64fgEsAXOm9XwvA5wCObElthRBCCCGEEELEyfWwZTZ27gcAngNwHIDRADp5779xzm0J4Fzv/U45+7f950UhhBBCCCGEaDuq7mGDc24p59xUAAsAjAUwC8AX3nvzp38AoEtqfyGEEEIIIYQQlVPWC5v3/lvvfV8AXQFsDqBXuSdwzh3jnJvknJuUv7UQQgghhBBCCKOiKJHe+y8AjAewJYAVnXO2erUrgA8T+4z03m9aidtPCCGEEEIIIUQZL2zOuQ7OuRVL9vcB7ABgOhpe3PYubTYMwIOLqpJCCCGEEEIIsSSydP4m6AzgVufcUmh4wbvLe/+Ic24agDudcxcAmALgpkVYTyGEEEIIIYRY4qgoSmSrT+bcxwC+AvBJzU4q6pn2UF8R+aifiHJRXxHlor4iykH9RJRL476yuve+Q7k71/SFDQCcc5O0nk2Ug/qKKAf1E1Eu6iuiXNRXRDmon4hyaW1fqSjoiBBCCCGEEEKI2qEXNiGEEEIIIYQoKG3xwjayDc4p6hP1FVEO6ieiXNRXRLmor4hyUD8R5dKqvlLzNWxCCCGEEEIIIcpDkkghhBBCCCGEKCh6YRNCCCGEEEKIglLTFzbn3GDn3FvOuZnOuTNreW5RbJxzs51zrznnpjrnJpXKVnbOjXXOvV36/0ptXU9Re5xzNzvnFjjnXqeyaN9wDfy2NMa86pzbuO1qLmpNoq+c65z7sDS2THXO7UJ/+2Wpr7zlnNupbWotao1zrptzbrxzbppz7g3n3Mmlco0rItBMP9GYIjI4577nnHvZOffXUl85r1Te3Tk3sdQn/uycW7ZU/t3Sv2eW/r5G3jlq9sLmnFsKwLUAdgawLoADnHPr1ur8oi7Yznvfl/JUnAlgnPe+J4BxpX+LJY9bAAxuVJbqGzsD6Fn67xgA/1ejOopicAua9hUAuLI0tvT13j8KAKX5Z38A65X2ua40T4nFn28ADPferwugH4ATSv1B44pgUv0E0JgisnwNYKD3fkMAfQEMds71A3AJGvrKWgA+B3BkafsjAXxeKr+ytF2z1NLDtjmAmd77d7z3/wFwJ4AhNTy/qD+GALi1ZN8KYI82rItoI7z3zwD4rFFxqm8MAXCbb+AlACs65zrXpqairUn0lRRDANzpvf/ae/8ugJlomKfEYo73fq73/i8l+x8ApgPoAo0rgmimn6TQmLKEUhobviz9c5nSfx7AQAD3lMobjyk21twDYJBzzjV3jlq+sHUB8D79+wM03/HFkoUH8IRzbrJz7phSWUfv/dySPQ9Ax7apmiggqb6hcUbEOLEkZbuZpNXqKwIlKdJGACZC44pI0KifABpTRCOcc0s556YCWABgLIBZAL7w3n9T2oT7Q+grpb//DUC75o6voCOiKGzlvd8YDdKTE5xz2/AffUP+CeWgEE1Q3xA5/B+AHmiQqcwFcHnbVkcUBefcDwHcC+AU7/3f+W8aV4QR6ScaU0QTvPffeu/7AuiKBs9qr2oev5YvbB8C6Eb/7loqEwLe+w9L/18A4H40dPb5Jjsp/X9B29VQFIxU39A4IzJ47+eXJtL/AbgBCyVK6itLMM65ZdDwEP4n7/19pWKNKyJDrJ9oTBHN4b3/AsB4AFuiQT69dOlP3B9CXyn9/UcAPm3uuLV8YXsFQM9SxJRl0bAw86Eanl8UFOfccs655c0GsCOA19HQP4aVNhsG4MG2qaEoIKm+8RCAQ0tR3foB+BtJnMQSSKO1RkPRMLYADX1l/1K0ru5oCCjxcq3rJ2pPaa3ITQCme++voD9pXBGBVD/RmCIa45zr4JxbsWR/H8AOaFjzOB7A3qXNGo8pNtbsDeCpklc/ydLN/bGaeO+/cc6dCOBxAEsBuNl7/0atzi8KTUcA95fWWy4N4A7v/Rjn3CsA7nLOHQlgDoB927COoo1wzo0CMABAe+fcBwB+DeBixPvGowB2QcNi738COLzmFRZtRqKvDHDO9UWDvG02gJ8CgPf+DefcXQCmoSEa3Ane+2/bot6i5vwYwCEAXiutOQGAs6BxRWRJ9ZMDNKaIRnQGcGspKuh3ANzlvX/EOTcNwJ3OuQsATEHDBwCU/v9H59xMNATK2j/vBC7nhU4IIYQQQgghRBuhoCNCCCGEEEIIUVD0wiaEEEIIIYQQBUUvbEIIIYQQQghRUPTCJoQQQgghhBAFRS9sQgghhBBCCFFQ9MImhBBCCCGEEAVFL2xCCCGEEEIIUVD+H04Ha/BC6x+KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# of the 64 images in the batches, we select the first 10 and display it \n",
    "# in a grid using torchvision.utils make_grid() function\n",
    "grid = torchvision.utils.make_grid(images[:10], nrow=10)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "\n",
    "print(\"labels\", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Neural Network Model\n",
    "\n",
    "We need to use, **torch.nn.Module** class. \n",
    "\n",
    "Class is like a blueprint, where people / things of similar nature / functionality exist. <br>\n",
    "### Methods and Attributes\n",
    "Objects are like the actual people / things which exist in a class. There can be multiple objects of a class, just like multiple students exist in a class. Those objects can perform all functions mentioned in the class. Simply put, all students can write, read and listen / understand to things taught in class. Yet each student reads / writes / listens in a different manner based on the inputs it receives or their own indifferences. When an object is created, we call it an **Instance** of a class. The Instance usually has 2 properties:<br>\n",
    "\n",
    "1. **Methods** : represent the code or behavior of the object. For example, a student can run, sing, write, read, eat, etc. These things are behaviour methods of a class. \n",
    "2. **Attributes** : they represent the data of the object. They hold the data which is to be used by the methods. Inshort they help us know the objects (student's) characteristics. For example, gender (male/female), age, grade level, athlete (yes/no), etc. They define characteristics of the object.\n",
    "<br><br>\n",
    "Recall, that a class is a **BluePrint** of the object. It does not contain actual information, but is customizable based on user information. **Methods** are standalone and consistent across the class, but the **Attributes** are customizable parameters.\n",
    "\n",
    "### Encapsulation\n",
    "An object has it's own set of attributes and can access all the methods of the class. An object therefore holds both information of attributes and methods, since it can access both of them uniquely. This packaging of attributes and methods into a single class is called **Encapsulation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack cannot play basketball\n",
      "Mike can play basketball!\n"
     ]
    }
   ],
   "source": [
    "class Students:\n",
    "    \n",
    "    def __init__(self, name, height):\n",
    "        self.name = name\n",
    "        self.height = height\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.height >= 6:\n",
    "            print(\"{} can play basketball!\".format(self.name))\n",
    "        else:\n",
    "            print(\"{} cannot play basketball\".format(self.name))\n",
    "    \n",
    "s = Students(\"Jack\", 5)\n",
    "s.play()\n",
    "s.name = \"Mike\"\n",
    "s.height = 6\n",
    "s.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheritance\n",
    "PyTorch has an inbuilt neural network library which is named torch.nn<br>\n",
    "There is a base class called **Module** which has all the functionality. i.e it has all the neural networks modules for constructing our model. This means all layers in PyTorch **extends** the nn.Module class. This concept of a class extending another class to gain access of it's attributes and methods is called **Inheritance**.\n",
    "<br><br>\n",
    "We inherit the **nn.Module** class to get all the functionality of the neural networks into our code. Inshort, if we imagine neural networks to be one large function, then layers are functions and our network is a **composition** of all those functions. Layers and networks are kind of similar in nature due to above analogy.\n",
    "\n",
    "### Understanding forward propagation\n",
    "A tensor passes through our network via each layer and undergoes transformation performed on it by each layer. This process of tensor flowing forward through the network is known as **forward pass** or **forward propagation**. Each layer has its own transformations and overall forward pass transformation of the network is the collection of all the transformations from these individual layers.<br>\n",
    "<br>\n",
    "Every **nn.Module** class has a **forward()** method which needs to be implemented in order to provide forward pass information. We need to implement the forward() method everytime we need to design our model.\n",
    "<br><br>\n",
    "When we implement the **forward()** function, we typically call the **nn.Functional** module. It provides packages for implementing our forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  # inheriting the nn.Module class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    \"\"\"\n",
    "    Implementation of forward function takes in a tensor t and transforms it using the dummy layer.\n",
    "    \"\"\"\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Module class is keeping track of the networks weights which are contained within each layer. \n",
    "<br>\n",
    "Linear layers are also called **Dense** layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get the network architecture here.\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a good string representation of our network object above. This is because we have extended the nn.Module class which prints this. If we remove it, then we get just an object reference of the Net() class like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Net at 0x7f1c24dfc518>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net():  # inheriting the nn.Module class\n",
    "    def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    \"\"\"\n",
    "    Implementation of forward function takes in a tensor t and transforms it using the dummy layer.\n",
    "    \"\"\"\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t\n",
    "\n",
    "## This is just used for illustration purposes. \n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **__repr__** we can have a valuable message to be displayed when we print an object. Special OOP methods in python are called **Dunder methods**. They are called Dunder because of **Double Underscore**.\n",
    "<br><br>\n",
    "Stride helps us define how much do we need to shift the kernel to the right and below during convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument vs Parameters\n",
    "\n",
    "Parameters are used inside function definition. They act as placeholders. They are like local variables.<br>\n",
    "Arguments are actual values which are passed into the functions. Values comes from outside the function to inside. \n",
    "<br>\n",
    "2 types of parameters enter the network:\n",
    "1. hyperparameter\n",
    "2. data dependent hyperparameters.\n",
    "<br>\n",
    "When we contruct a layer we pass parameters and values to the layers. For convolutional layers we have 3 parameters and for FC layers we have 2 parameters.\n",
    "<br>\n",
    "\n",
    "**Hyperparameters** values are chosen manually or arbitrarily. Manually choosing hyperparameters include **kernel_size**, **out_channels** and **out_features**, etc.\n",
    "<br><br>\n",
    "Data dependent hyperparameters include, **input channels** which is equal to depth of the image (either 1 or 3) and **output features** (i.e number of classes in the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters vs Kernels\n",
    "\n",
    "A kernel is a small nxn matrix which traverses over the image. They are usually odd numbered (preferably, like 3x3, 5x5, 7x7, etc) but can be even numbered too like 2x2 or 4x4.\n",
    "<br><br>\n",
    "A filter is basically a collection of kernels. \n",
    "<br><br>\n",
    "**Output channels** are equal to **# of filters**. They are commonly known as **Feature Maps**. If we are dealing with linear layers we don't call them feature maps, since they are a rank 1 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnable parameters\n",
    "\n",
    "Learnable parameters are parameters whose values are trained during training process. We typically start with an arbitrary set of values and they get updated iteratively as the network learns.\n",
    "<br><br>\n",
    "Our objective is to train the network so that it can **learn** the best set of these parameters in order to **minimize** the error it makes (i.e loss function)\n",
    "<br><br>\n",
    "These parameters are commonly called **weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):  # inheriting the nn.Module class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    \"\"\"\n",
    "    Implementation of forward function takes in a tensor t and transforms it using the dummy layer.\n",
    "    \"\"\"\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t\n",
    "    \n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=192, out_features=120, bias=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=60, bias=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=60, out_features=10, bias=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the network object returns a string **representation(__repr__)** of the object, we can get individual layer informations as well using the **dot(.)** notation. Lets look at the initialized weights inside this functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0366, -0.0529, -0.0544, -0.1039, -0.0145],\n",
       "          [ 0.0435, -0.0871,  0.0871, -0.0570,  0.0163],\n",
       "          [ 0.0150, -0.0685, -0.0948,  0.0476,  0.0396],\n",
       "          [ 0.1088, -0.0301, -0.0944, -0.0432, -0.0810],\n",
       "          [-0.0584,  0.0457, -0.1038, -0.1140, -0.0893]],\n",
       "\n",
       "         [[ 0.0818, -0.0213, -0.0510, -0.0255,  0.0568],\n",
       "          [ 0.1064, -0.0088,  0.0519, -0.0760,  0.1115],\n",
       "          [-0.0673,  0.0254, -0.0066, -0.0609, -0.1087],\n",
       "          [-0.0852, -0.0207,  0.1143, -0.0391, -0.0437],\n",
       "          [ 0.0175,  0.0110, -0.1077,  0.0396,  0.0775]],\n",
       "\n",
       "         [[ 0.0657,  0.1002, -0.1006,  0.0329, -0.0757],\n",
       "          [-0.0391,  0.0189,  0.0920,  0.0622, -0.0661],\n",
       "          [-0.0503,  0.0649,  0.0443, -0.0931,  0.0922],\n",
       "          [-0.0396, -0.1120, -0.0791,  0.0810, -0.0192],\n",
       "          [ 0.0907,  0.1060, -0.0251,  0.0452, -0.1018]]],\n",
       "\n",
       "\n",
       "        [[[-0.0610,  0.0050, -0.0174, -0.0181, -0.0592],\n",
       "          [-0.0520, -0.0900,  0.1116, -0.0051,  0.1070],\n",
       "          [ 0.0542, -0.1114,  0.1144, -0.0131,  0.0767],\n",
       "          [-0.0653,  0.0764,  0.0207,  0.0240,  0.0524],\n",
       "          [-0.0969, -0.0064, -0.0806,  0.0285, -0.0954]],\n",
       "\n",
       "         [[ 0.1050,  0.0233,  0.0735,  0.0983, -0.0712],\n",
       "          [ 0.0758, -0.0965,  0.0564, -0.0894,  0.0085],\n",
       "          [-0.0750, -0.0456,  0.0002,  0.0503,  0.0499],\n",
       "          [ 0.0288,  0.0596, -0.0261, -0.0452, -0.0021],\n",
       "          [-0.0948,  0.0365, -0.1145, -0.0722, -0.0288]],\n",
       "\n",
       "         [[-0.0049, -0.0426, -0.0700, -0.0885,  0.1096],\n",
       "          [ 0.0993,  0.0786, -0.0042,  0.0285,  0.0293],\n",
       "          [-0.0750,  0.0310, -0.0124, -0.0623,  0.0894],\n",
       "          [-0.0465,  0.0475, -0.0208,  0.0352,  0.0987],\n",
       "          [-0.0754,  0.0515, -0.0883,  0.0433,  0.0311]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1085, -0.0135, -0.0419, -0.1144,  0.1020],\n",
       "          [-0.0182, -0.0709,  0.0359,  0.0822, -0.0258],\n",
       "          [-0.0589,  0.0264, -0.0032, -0.0950,  0.0466],\n",
       "          [ 0.0036,  0.0598, -0.1091,  0.1152,  0.0375],\n",
       "          [-0.0163,  0.0753, -0.0853,  0.1029,  0.0063]],\n",
       "\n",
       "         [[ 0.0435, -0.0490, -0.0930,  0.1048,  0.0315],\n",
       "          [-0.0598, -0.0672,  0.0339, -0.0766, -0.0634],\n",
       "          [ 0.0507,  0.0764,  0.0885, -0.0848,  0.0360],\n",
       "          [ 0.0802,  0.0489, -0.0522,  0.0184, -0.0887],\n",
       "          [-0.0005, -0.0540, -0.0201, -0.1020, -0.0800]],\n",
       "\n",
       "         [[-0.0511, -0.0167, -0.0202, -0.0305, -0.0442],\n",
       "          [-0.0747,  0.1112,  0.0975, -0.0941, -0.0331],\n",
       "          [-0.0635,  0.0707,  0.0995,  0.0148, -0.0855],\n",
       "          [-0.0471,  0.0199, -0.1119,  0.0731, -0.0666],\n",
       "          [-0.0293,  0.0179,  0.0968, -0.0900,  0.1008]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0364,  0.1153,  0.0268,  0.0126, -0.0195],\n",
       "          [ 0.0183, -0.1088, -0.0678,  0.0313, -0.0944],\n",
       "          [ 0.0373, -0.0322, -0.0070,  0.1069,  0.0971],\n",
       "          [ 0.1019,  0.0827,  0.1010,  0.0724,  0.0482],\n",
       "          [ 0.0446,  0.0561, -0.0347, -0.0514,  0.0747]],\n",
       "\n",
       "         [[-0.0538, -0.0822,  0.0186,  0.0383, -0.0890],\n",
       "          [-0.1098,  0.0094, -0.0159, -0.0247,  0.0412],\n",
       "          [-0.0446,  0.1043, -0.0421,  0.0636, -0.0078],\n",
       "          [ 0.0550, -0.0117, -0.0496, -0.1143, -0.0306],\n",
       "          [-0.1093,  0.0154, -0.0508,  0.1109, -0.0306]],\n",
       "\n",
       "         [[ 0.0939, -0.0098, -0.0289,  0.1036,  0.0500],\n",
       "          [-0.0771, -0.0542, -0.0950, -0.0592,  0.0992],\n",
       "          [ 0.0004,  0.0897,  0.0233,  0.0715,  0.1061],\n",
       "          [ 0.0085,  0.1052, -0.0207, -0.0622, -0.0880],\n",
       "          [-0.0635,  0.0997, -0.0902, -0.0891, -0.1025]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0305, -0.1048, -0.1127, -0.0214, -0.0599],\n",
       "          [-0.0562, -0.0737, -0.0535, -0.0058, -0.0595],\n",
       "          [ 0.0471, -0.0349, -0.0922,  0.0990,  0.0159],\n",
       "          [ 0.0939, -0.0015, -0.1104, -0.0676,  0.1146],\n",
       "          [ 0.0354,  0.1102,  0.0400,  0.1055,  0.0265]],\n",
       "\n",
       "         [[-0.0487,  0.0873,  0.1090,  0.0661, -0.0706],\n",
       "          [-0.0728, -0.0085,  0.0410,  0.0148,  0.0777],\n",
       "          [-0.0276, -0.0913, -0.0009,  0.0974,  0.0099],\n",
       "          [ 0.0632, -0.0092,  0.1132,  0.0636, -0.0169],\n",
       "          [-0.0813,  0.1014,  0.0626, -0.0686,  0.0227]],\n",
       "\n",
       "         [[ 0.0810,  0.0166,  0.1116,  0.0532,  0.0445],\n",
       "          [ 0.0683, -0.0637,  0.0117, -0.0367,  0.1033],\n",
       "          [-0.0582, -0.0731, -0.0883, -0.0852, -0.0803],\n",
       "          [-0.0839, -0.0842, -0.0423, -0.0968,  0.1134],\n",
       "          [-0.0004, -0.0532,  0.1082, -0.0731,  0.0768]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0193,  0.0065,  0.1037,  0.0165,  0.1090],\n",
       "          [ 0.0142, -0.1008,  0.0083,  0.1078, -0.0780],\n",
       "          [ 0.0004,  0.0562, -0.1114,  0.0667,  0.0918],\n",
       "          [-0.0181,  0.0675,  0.0550,  0.0043,  0.0545],\n",
       "          [-0.0077,  0.1098, -0.0959,  0.0609, -0.0052]],\n",
       "\n",
       "         [[-0.0973,  0.0451,  0.0012, -0.0069, -0.0395],\n",
       "          [-0.0439, -0.0602, -0.0104, -0.0317, -0.0468],\n",
       "          [-0.0238,  0.0216,  0.0130,  0.0997,  0.0404],\n",
       "          [-0.0119,  0.0610, -0.0377,  0.0051,  0.1043],\n",
       "          [-0.0084,  0.0525,  0.0628,  0.0166,  0.0203]],\n",
       "\n",
       "         [[ 0.0101,  0.0282,  0.0541, -0.1003, -0.0345],\n",
       "          [ 0.0851,  0.0044,  0.0472,  0.0786,  0.0859],\n",
       "          [ 0.0860,  0.0287,  0.0925, -0.1010,  0.0771],\n",
       "          [ 0.0521, -0.0504, -0.0248, -0.0851, -0.1063],\n",
       "          [-0.0909,  0.0522,  0.0479,  0.0781, -0.0435]]]], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output proves that each filter has different kernels. We can see, each filter has 3 kernels (each unique in nature) of size 5x5. They are randomly initialized. These 3 kernels together make up a filter. Since a filter has 3 kernels and there are 6 filters, we have total of 18 kernels. Since image is a color image, a filter has 3 kernels each corresponding to an RGB channel. If grayscale, then each filter will have only 1 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 5, 5])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.out.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers weight matrix called a linear map. It maps a higher dimensional space into lower dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
